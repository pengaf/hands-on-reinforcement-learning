{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, gae_lambda, repetition_epochs, clip_epsilon, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),lr = actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr = critic_lr)\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.repetition_epochs = repetition_epochs\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def compute_advantage(self, td_delta):\n",
    "        td_delta = td_delta.detach().numpy()\n",
    "        advantages = torch.zeros(len(td_delta))\n",
    "        advantage = 0\n",
    "        for n in reversed(range(len(td_delta))):\n",
    "            delta = td_delta[n][0]\n",
    "            #print(type(td_delta),td_delta)\n",
    "            advantage = advantage * self.gamma * self.gae_lambda + delta\n",
    "            advantages[n] = advantage\n",
    "        return advantages.view(-1,1)\n",
    "\n",
    "    def update(self, states, actions, rewards, next_states, nonterminals):\n",
    "        states = torch.tensor(states, dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(actions).view(-1,1).to(self.device)\n",
    "        rewards = torch.tensor(rewards,dtype=torch.float).view(-1,1).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float).to(self.device)\n",
    "        nonterminals = torch.tensor(nonterminals, dtype=torch.float).view(-1,1).to(self.device)\n",
    "        \n",
    "        values = self.critic(states)\n",
    "        next_values = self.critic(next_states)\n",
    "        td_target = rewards + self.gamma * next_values * nonterminals\n",
    "        td_delta = td_target - values\n",
    "        advantage = self.compute_advantage(td_delta)\n",
    "\n",
    "        old_log_probs = torch.log(self.actor(states).gather(1, actions)).detach()\n",
    "\n",
    "        for _ in range(self.repetition_epochs):\n",
    "            log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clip(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "            actor_loss = torch.mean(-torch.min(surr1, surr2))\n",
    "            critic_loss = torch.mean(torch.nn.functional.mse_loss(values, td_target.detach()))\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-2\n",
    "\n",
    "hidden_dim = 128\n",
    "gamma = 0.98\n",
    "lmbda = 0.95\n",
    "epochs = 10\n",
    "epsilon = 0.2\n",
    "\n",
    "device = torch.device(\"cpu\") #torch.device(\"cuda\") if torch.cuda.is_available () else torch.device(\"cpu\")\n",
    "env_name = \"CartPole-v0\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "env.close()\n",
    "\n",
    "def reset_seed():\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "def train(agent, num_episodes):\n",
    "    reset_seed()\n",
    "    reward_stat = np.zeros(num_episodes)\n",
    "    env = gym.make(env_name)\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset(seed=0)\n",
    "        total_reward = 0\n",
    "        states=[]\n",
    "        actions=[]\n",
    "        rewards=[]\n",
    "        next_states=[]\n",
    "        nonterminals=[]\n",
    "        while True:\n",
    "            action = agent.take_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            nonterminals.append(not terminated)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        agent.update(states, actions, rewards, next_states, nonterminals)\n",
    "        if episode*10 % num_episodes == 0:\n",
    "            print(\"episode:\", episode, \"total_reward:\",total_reward)\n",
    "        reward_stat[episode] = total_reward\n",
    "    env.close()\n",
    "    return reward_stat\n",
    "\n",
    "def test(agent, num_episodes):\n",
    "    env = gym.make(env_name,render_mode=\"human\")\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = agent.take_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        #print(\"episode:\", episode, \"total_reward:\",total_reward)\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def moving_average(nums, n):\n",
    "    beta = 1 - 1.0/n\n",
    "    alpha = 1.0 - beta\n",
    "    avg = nums[0]\n",
    "    res = np.zeros_like(nums)\n",
    "    for n in range(len(nums)):\n",
    "        avg = avg * beta + nums[n]*alpha\n",
    "        res[n] = avg\n",
    "    return res        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mg:\\pengaf\\hands-on-reinforcement-learning\\ppo.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m reset_seed()\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m agent1 \u001b[39m=\u001b[39m PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, epochs, epsilon, gamma, device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m reward_stat1  \u001b[39m=\u001b[39m train(agent1, num_episodes)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(reward_stat1)), moving_average(reward_stat1,\u001b[39m10\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;32mg:\\pengaf\\hands-on-reinforcement-learning\\ppo.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m     \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m agent\u001b[39m.\u001b[39;49mupdate(states, actions, rewards, next_states, nonterminals)\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m \u001b[39mif\u001b[39;00m episode\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m \u001b[39m%\u001b[39m num_episodes \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mepisode:\u001b[39m\u001b[39m\"\u001b[39m, episode, \u001b[39m\"\u001b[39m\u001b[39mtotal_reward:\u001b[39m\u001b[39m\"\u001b[39m,total_reward)\n",
      "\u001b[1;32mg:\\pengaf\\hands-on-reinforcement-learning\\ppo.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m actor_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m critic_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/pengaf/hands-on-reinforcement-learning/ppo.ipynb#W1sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "num_episodes = 800\n",
    "reset_seed()\n",
    "agent1 = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, epochs, epsilon, gamma, device)\n",
    "reward_stat1  = train(agent1, num_episodes)\n",
    "plt.plot(range(len(reward_stat1)), moving_average(reward_stat1,10))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
